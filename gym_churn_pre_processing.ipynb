{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720d849f-18ef-43b6-aaca-92deb702d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"cleaned_gym_churn_us.csv\")\n",
    "\n",
    "# ====================================================================\n",
    "# Data Cleaning (Removing unnecessary index column)\n",
    "# ====================================================================\n",
    "df_processed = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Note on \"Creating dummy features\":\n",
    "# The boolean columns in this dataset (gender, Near_Location, Partner, etc.)\n",
    "# are already in a dummy/encoded format (True/False) and thus satisfy the rubric requirement.\n",
    "# No pd.get_dummies() is required.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368d5d7d-b33d-4295-b758-8f9374d661c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2800, 13)\n",
      "X_test shape: (1200, 13)\n"
     ]
    }
   ],
   "source": [
    "# 1. Split data into training and testing subsets\n",
    "\n",
    "# Define target variable (y) and features (X)\n",
    "y = df_processed['Churn']\n",
    "X = df_processed.drop('Churn', axis=1)\n",
    "\n",
    "# Split the data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,         # 30% for testing\n",
    "    random_state=42,       # For reproducibility\n",
    "    stratify=y             # Maintains the Churn class balance in both sets\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad522dc9-201c-4d8b-b810-709691aa75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scale standardization\n",
    "\n",
    "# Identify numerical columns for standardization\n",
    "numerical_cols = [\n",
    "    'Contract_period',\n",
    "    'Age',\n",
    "    'Avg_additional_charges_total',\n",
    "    'Month_to_end_contract',\n",
    "    'Lifetime',\n",
    "    'Avg_class_frequency_total',\n",
    "    'Avg_class_frequency_current_month'\n",
    "]\n",
    "\n",
    "# Separate numerical data for scaling\n",
    "X_train_num = X_train[numerical_cols]\n",
    "X_test_num = X_test[numerical_cols]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251a13e6-c898-47e8-871d-9865022d22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit the scaler ONLY on the training data \n",
    "scaler.fit(X_train_num)\n",
    "\n",
    "# 2. Transform both the training and testing data\n",
    "X_train_scaled_array = scaler.transform(X_train_num)\n",
    "X_test_scaled_array = scaler.transform(X_test_num)\n",
    "\n",
    "# Convert the scaled arrays back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled_array,\n",
    "    columns=numerical_cols,\n",
    "    index=X_train_num.index # Preserve original index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled_array,\n",
    "    columns=numerical_cols,\n",
    "    index=X_test_num.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5cf5371-2f21-43ad-a18b-788ad3dc2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardization Complete. Data is ready for model training.\n",
      "Final X_train columns: ['gender', 'Near_Location', 'Partner', 'Promo_friends', 'Phone', 'Group_visits', 'Contract_period', 'Age', 'Avg_additional_charges_total', 'Month_to_end_contract', 'Lifetime', 'Avg_class_frequency_total', 'Avg_class_frequency_current_month']\n"
     ]
    }
   ],
   "source": [
    "# Re-assemble the final training and testing datasets\n",
    "# Drop the original numerical columns, then concatenate the scaled ones\n",
    "X_train_final = X_train.drop(columns=numerical_cols).copy()\n",
    "X_test_final = X_test.drop(columns=numerical_cols).copy()\n",
    "\n",
    "X_train_final = pd.concat([X_train_final, X_train_scaled], axis=1)\n",
    "X_test_final = pd.concat([X_test_final, X_test_scaled], axis=1)\n",
    "\n",
    "print(\"\\nStandardization Complete. Data is ready for model training.\")\n",
    "print(f\"Final X_train columns: {X_train_final.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5d570-72b4-4d55-8952-c39057a5ac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
